{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqeTXMBC9VcW"
      },
      "source": [
        "## for first script\n",
        "!pip install pypdf2\n",
        "\n",
        "## for second script\n",
        "# installing the spanish model by spacy\n",
        "# !python -m spacy download es_core_news_md\n",
        "!spacy download es_core_news_md\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sd1FoXo8ez6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f22dcd-b879-45f6-ce93-1e107e8de44f"
      },
      "source": [
        "# This script will extract the data from the pdf\n",
        "\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "def extract_text() :\n",
        "    # Reading the PDF using PyPDF2\n",
        "    reader = PyPDF2.PdfFileReader(\"./data/main_data.pdf\")\n",
        "    text = \"\"\n",
        "\n",
        "    # This will extract the text from all the pages (21 to 401)\n",
        "    # and will also replace new line with no-space\n",
        "    for i in range(21, 401) :\n",
        "        page_text = reader.getPage(i).extractText()\n",
        "        text += page_text.replace(\"\\n\", \"\")\n",
        "\n",
        "    # This will remove all extra white spaces and for this,\n",
        "    # we are making use of REGEX i.e. \" +\", this means 1 or\n",
        "    # more than 1 space\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    # Saving the extracted text into a .txt file\n",
        "    with open(\"./files/transcript_clean.txt\", \"w\", encoding=\"utf-8\") as temp_file :\n",
        "        temp_file.write(text)\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    extract_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFPR0weX-YCg"
      },
      "source": [
        "import csv\n",
        "import spacy\n",
        "\n",
        "def loadModel() :\n",
        "  # Now we will define our corpus (a term used for resource consisting large and structured set of text)\n",
        "  # by reading the clean file, which we created earlier by extracting text\n",
        "  text_corpus = open(\"./files/transcript_clean.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "  nlp = spacy.load('es_core_news_md') # Loading the spanish model file which we downloaded\n",
        "\n",
        "  # Now, maximum length of this loaded spacy file is less than our clean file, so, we'll increase its max\n",
        "  # length and will do it equal to our corpus's length\n",
        "  # Spacy has a max_length limit of 1,000,000 characters. (via. StackOverflow)\n",
        "  nlp.max_length = len(text_corpus)\n",
        "\n",
        "  # Now we will create a document (Term generally used for NLP processed text) by passing our corpus through \n",
        "  # our NLP pipeline so that we can process on that data\n",
        "  doc = nlp(text_corpus)\n",
        "\n",
        "  getTokens(doc) # Extracting and segmenting the tokens (words, punctuations etc.) from the text\n",
        "  countPosNeg(doc) # Counting the positive and negative words per sentence\n",
        "\n",
        "def getTokens(doc) :\n",
        "  # This will be our list of tokens i.e. we will extract all the tokens and append it to this list\n",
        "  # All these \"attributes\" (text, text_lower, lemma, part_of_speech etc.) are provided by spaCy library, which\n",
        "  # are used to classify the tokens as given\n",
        "  # Here, we have appended a list of strings, this will be used to classify each token's attribute, once it is\n",
        "  # written in the CSV file\n",
        "  tokenList = [[\"text\", \"text_lower\", \"lemma\", \"lemma_lower\", \"part_of_speech\", \"is_alphabet\", \"is_stopword\"]]\n",
        "\n",
        "  # Appending each token (its attributes) in the list of tokens\n",
        "  # Here, _ after the attributes are used for unicode type instead of int\n",
        "  for token in doc :\n",
        "    tokenList.append([token.text, token.lower_, token.lemma_, token.lemma_.lower(), token.pos_, token.is_alpha, token.is_stop])\n",
        "\n",
        "  # Finally we will put the tokens inside a file in CSV format\n",
        "  with open(\"./files/tokens.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as tokensFile :\n",
        "    csv.writer(tokensFile).writerows(tokenList)\n",
        "\n",
        "def countPosNeg(doc) :\n",
        "  # This function will count the positive and negative words per sentence by using the DATASET from KAGGLE\n",
        "  # The DATASET negative_words_es.txt contains all the negative (sentiment) words\n",
        "  # The DATASET positive_words_es.txt contains all the positive (sentiment) words\n",
        "\n",
        "  # Reading the words from both files and converting them into a list of words\n",
        "  with open(\"./data/positive_words_es.txt\", \"r\", encoding=\"utf-8\") as posWordsFile:\n",
        "    positive_words = posWordsFile.read().splitlines()\n",
        "\n",
        "  with open(\"./data/negative_words_es.txt\", \"r\", encoding=\"utf-8\") as negWordsFile:\n",
        "    negative_words = negWordsFile.read().splitlines()\n",
        "\n",
        "  # This list will contain the words with their specific scores classifying them into positive and negative words\n",
        "  scoreList = [[\"text\", \"score\"]]\n",
        "\n",
        "  # Iterating over each sentence in our corpus\n",
        "  for sentence in doc.sents :\n",
        "    # Technically a sentence is a combination of many words that's why we will only consider it as a sentence if and only if \n",
        "    # its length is greater than 10\n",
        "    if len(sentence.text) > 10 :\n",
        "      score = 0\n",
        "      # For each sentence we will match each word with the list of negative and positive words that we have \n",
        "      # and will mark the score for each sentence\n",
        "      for word in sentence :\n",
        "        if word.lower_ in positive_words:\n",
        "          score += 1\n",
        "\n",
        "        if word.lower_ in negative_words:\n",
        "          score -= 1\n",
        "\n",
        "    scoreList.append([sentence.text, score])\n",
        "\n",
        "  # Finally we will store all the scores one by one in a CSV file\n",
        "  with open(\"./files/sentences.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as sentencesFile:\n",
        "    csv.writer(sentencesFile).writerows(scoreList)\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    loadModel()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDhptPQWAguD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8db868-4b9e-427b-82e6-c2b66dba42d6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def getWordsCount(tokensDF) :\n",
        "  # In the given dataframe of tokens, \"programas\" (programs) lemma is not correct, which is resulting in\n",
        "  # incorrect output so, we will group together, programas and programar\n",
        "  # WORKING : first of all it will get the lemma_lower token and for all its values in the dataframe, it \n",
        "  # will check if that is equal to \"programa\" and after that, it'll go only in the lemma_lower label and\n",
        "  # change all instances of \"programa\" to \"programar\"\n",
        "  tokensDF.loc[tokensDF['lemma_lower'] == \"programa\", \"lemma_lower\"] = \"programar\"\n",
        "  \n",
        "  words = tokensDF[tokensDF[\"is_alphabet\"] == True][\"text_lower\"].count()\n",
        "  print(\"Words : \", words)\n",
        "\n",
        "  unique_words = tokensDF[tokensDF[\"is_alphabet\"] == True][\"lemma_lower\"].nunique()\n",
        "  print(\"Unique words : \", unique_words)\n",
        "\n",
        "def plotWordCount(tokensDF) :\n",
        "  # We will only plot the graph by using the top 20 words which are not stop words and also they must be\n",
        "  # greater than 1 character\n",
        "  # Changing all the instances of programa with programar\n",
        "  tokensDF.loc[tokensDF['lemma_lower'] == \"programa\", \"lemma_lower\"] = \"programar\"\n",
        "\n",
        "  # First of all we will check all the conditions i.e. token must be alphabet, it must not be a stopword\n",
        "  # and it must be of length > 1\n",
        "  # Then, we will count all those words and will only take first 20 occurences of these type of words\n",
        "  words = tokensDF[(tokensDF[\"is_alphabet\"] == True) & \n",
        "                   (tokensDF[\"is_stopword\"] == False) & \n",
        "                   (tokensDF[\"lemma_lower\"].str.len() > 1)][\"lemma_lower\"].value_counts()[:20]\n",
        "\n",
        "  # Finally plotting the graph using sns i.e. Seaborn library object and matplotlib\n",
        "  # We are using plt.subplots(size) which will return a tuple containing the figure itself and axes object\n",
        "  # which we will assign to the variables fig and ax\n",
        "  # Setting the color of the text of whole graph\n",
        "  plt.rcParams[\"text.color\"] = \"black\"\n",
        "  plt.rcParams[\"axes.labelcolor\"] = \"black\"\n",
        "  plt.rcParams[\"xtick.color\"] = \"black\"\n",
        "  plt.rcParams[\"ytick.color\"] = \"black\"\n",
        "  fig, ax = plt.subplots(figsize=(11,7))\n",
        "  sns.barplot(x=words.values, y=words.index, ax=ax, palette=\"gist_heat_d\", linewidth=0)\n",
        "  # gridlines OFF\n",
        "  ax.grid(False)\n",
        "  # Setting the limit of x-axis as 0 -- 900\n",
        "  ax.set_xlim(0, 900)\n",
        "  # Here, we are setting the ticks (i.e. values on the x-axis)\n",
        "  ax.set_xticks(range(0, 901, 100))\n",
        "  # Setting the background color\n",
        "  ax.set_facecolor(\"#fbddc8\")\n",
        "  # Setting the border color of the box\n",
        "  ax.spines[\"bottom\"].set_color(\"black\")\n",
        "  ax.spines[\"left\"].set_color(\"black\")\n",
        "  ax.spines[\"right\"].set_color(\"black\")\n",
        "  ax.spines[\"top\"].set_color(\"black\")\n",
        "  # Label on the x-axis\n",
        "  plt.xlabel(\"Occurrences Count\")\n",
        "  plt.ylabel(\"Words\")\n",
        "  # Title of the graph\n",
        "  plt.title(\"Most Frequent Words\")\n",
        "  # Saving the plotted graph image\n",
        "  plt.savefig(\"./images/words_count.png\", facecolor=\"#fbddc8\")\n",
        "  plt.close()\n",
        "\n",
        "def plotSentiment(sentencesDF) :\n",
        "  # We are plotting the graph for the scores between -10 to 10\n",
        "  sentencesDF = sentencesDF[(sentencesDF[\"score\"] >= -10) & (sentencesDF[\"score\"] <= 10)]\n",
        "  # Setting the color of the text of whole graph\n",
        "  plt.rcParams[\"text.color\"] = \"white\"\n",
        "  plt.rcParams[\"axes.labelcolor\"] = \"white\"\n",
        "  plt.rcParams[\"xtick.color\"] = \"white\"\n",
        "  plt.rcParams[\"ytick.color\"] = \"white\"\n",
        "  # Setting the size of the graph and extracting the values of fig and axes\n",
        "  fig, ax = plt.subplots(figsize=(11,7))\n",
        "  # Arranging the labels for y-axis\n",
        "  yLabels = [str(i) for i in range(-12, 12, 2)]\n",
        "  plt.yticks(np.arange(-12, 12, 2), yLabels)\n",
        "  # Scores below 0 will be colored with RED and above 0 colored with GREEN for a better understanding and\n",
        "  # making it distinguishable to get the idea of positive and negative words\n",
        "  # For doing this, first we will have to make an array, and for this we will take help of numpy\n",
        "  # Each value in the array is a 3-tuple representing RGB\n",
        "  # And the length of the array will be equal to the length of our DF\n",
        "  colors = np.array([\"#fd0054\"] * len(sentencesDF[\"score\"]))\n",
        "  colors[sentencesDF[\"score\"] >= 0] = [\"#4ef037\"] # Changing the color of the scores >= 0 (i.e. +ve)\n",
        "  # Plotting the bars\n",
        "  plt.bar(sentencesDF.index, sentencesDF[\"score\"], color = colors, linewidth = 0)\n",
        "  # grids OFF\n",
        "  ax.grid(False)\n",
        "  # Setting the background color\n",
        "  ax.set_facecolor(\"#222831\")\n",
        "  # Setting the border color of the box\n",
        "  ax.spines[\"bottom\"].set_color(\"white\")\n",
        "  ax.spines[\"left\"].set_color(\"white\")\n",
        "  ax.spines[\"right\"].set_color(\"white\")\n",
        "  ax.spines[\"top\"].set_color(\"white\")\n",
        "  # Labelling x-axis, y-axis and title\n",
        "  plt.xlabel(\"Sentence Number\")\n",
        "  plt.ylabel(\"Score\")\n",
        "  plt.title(\"Sentiment Analysis (Positive and Negative)\")\n",
        "  # Saving the image\n",
        "  plt.savefig(\"./images/sentiment.png\", facecolor=\"#222831\")\n",
        "  plt.close()\n",
        "\n",
        "def overallSentiment(sentencesDF) :\n",
        "  # Considering only the scores between -10 to 10\n",
        "  sentencesDF = sentencesDF[(sentencesDF[\"score\"] >= -10) & (sentencesDF[\"score\"] <= 10)]\n",
        "  # Finding the total score (of sentiment)\n",
        "  totalScore = sentencesDF[\"score\"].sum()\n",
        "  print(\"Total (sum) score of the sentiment :\", totalScore, end=\"\\n\\n\")\n",
        "  # Sentences with neutral sentiment\n",
        "  neutralCount = sentencesDF[sentencesDF[\"score\"] == 0][\"score\"].count()\n",
        "  # Sentences with positive sentiment\n",
        "  positiveCount = sentencesDF[(sentencesDF[\"score\"] > 0) & (sentencesDF[\"score\"] <= 10)][\"score\"].count()\n",
        "  # Sentences with negative sentiment\n",
        "  negativeCount = sentencesDF[(sentencesDF[\"score\"] >= -10) & (sentencesDF[\"score\"] < 0)][\"score\"].count()\n",
        "  print(\"Neutral count : {}\\nPositive count : {}\\nNegative count : {}\\n\".format(neutralCount, positiveCount, negativeCount))\n",
        "\n",
        "  if negativeCount > positiveCount :\n",
        "    if negativeCount > neutralCount :\n",
        "      print(\"Overall Sentiment of PDF is : Negative\")\n",
        "    else :\n",
        "      print(\"Overall Sentiment of PDF is : Neutral\")\n",
        "  elif positiveCount > negativeCount :\n",
        "    if positiveCount > neutralCount :\n",
        "      print(\"Overall sentiment of PDF is : Positive\")\n",
        "    else :\n",
        "      print (\"Overall sentiment of PDF is : Neutral\")\n",
        "  else :\n",
        "    print(\"Overall sentiment of PDF is : Neutral\")\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "  # First of all we will import all the data from CSV files i.e. tokens.csv and sentences.csv\n",
        "  # by using pandas library and will create its dataframe\n",
        "  tokensDF = pd.read_csv(\"./files/tokens.csv\")\n",
        "  sentencesDF = pd.read_csv(\"./files/sentences.csv\")\n",
        "\n",
        "  ## getWordsCount(tokensDF) # Just for finding the total count of words and unique words\n",
        "  plotWordCount(tokensDF) # Plotting the graph of occurrences of most used words\n",
        "  plotSentiment(sentencesDF) # Plotting the sentiment analysis of the whole PDF which was done by calculating number of positive and negative words per sentence\n",
        "  overallSentiment(sentencesDF) # Finding the total sentiment of the whole PDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total (sum) score of the sentiment : -3929\n",
            "\n",
            "Neutral count : 1330\n",
            "Positive count : 944\n",
            "Negative count : 2604\n",
            "\n",
            "Overall Sentiment of PDF is : Negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}